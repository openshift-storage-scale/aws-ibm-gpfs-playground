---
- name: Playbook to set up the Openshift Cluster
  hosts: localhost
  gather_facts: false
  become: false
  vars:
    use_operator: true
  vars_files:
    # Use this to override stuff that won't be committed to git
    - ../overrides.yml
  tasks:
    - name: Create working folder
      tags:
        - 1_ocp_install
      ansible.builtin.file:
        path: "{{ ocpfolder }}"
        state: directory
        recurse: true

    - name: Does cluster metadata.json exist
      tags:
        - 1_ocp_install
      ansible.builtin.stat:
        path: "{{ ocpfolder }}/metadata.json"
      register: metadata_json_file

    - name: Template OCP install file
      tags:
        - 1_ocp_install
      ansible.builtin.template:
        src: ../templates/full-cluster-install-config.j2.yaml
        dest: "{{ ocpfolder }}/install-config.yaml"
      when: not metadata_json_file.stat.exists

    - name: Set kubeadmin password fact
      tags:
        - 1_ocp_install
      ansible.builtin.shell: |
        python -c 'import bcrypt; print(bcrypt.hashpw(b"{{ kubeadmin_pass }}", bcrypt.gensalt(rounds=10)).decode())' | base64 -w0
      register:
        hashed_password_out

    - name: Set hashed password fact
      tags:
        - 1_ocp_install
      ansible.builtin.set_fact:
        hashed_password: "{{ hashed_password_out.stdout }}"

    - name: Install ocp cluster
      tags:
        - 1_ocp_install
      ansible.builtin.shell: |
        set -ex
        id
        {{ basefolder }}/{{ ocp_version }}/openshift-install create cluster --dir=. &> /tmp/oc-{{ ocp_version }}-{{ ocp_cluster_name }}.log
      args:
        chdir: "{{ ocpfolder }}"
      when: not metadata_json_file.stat.exists

    - name: Does cluster kubeconfig exist
      tags:
        - 1_ocp_install
      ansible.builtin.stat:
        path: "{{ ocpfolder }}/auth/kubeconfig"
      register: kubeconfig_file

    - name: Fail here there is no kubeconfig file
      tags:
        - 1_ocp_install
      fail: msg="The openshift cluster kubeconfig file is missing, please check {{ ocpfolder }}.openshift_install.log"
      when: not kubeconfig_file.stat.exists

    - name: Set kubeadmin password
      tags:
        - 1_ocp_install
      ansible.builtin.shell: |
        set -e
        chmod 0600 ./auth/kubeconfig
        export KUBECONFIG=./auth/kubeconfig
        {{ oc_bin }} patch secret -n kube-system kubeadmin --type json -p '[{"op": "replace", "path": "/data/kubeadmin", "value": "'{{ hashed_password }}'"}]'
      args:
        chdir: "{{ ocpfolder }}"

    - name: Template mco file
      tags:
        - 1_ocp_install_mco
      ansible.builtin.template:
        src: ../templates/mco.yaml
        dest: "{{ gpfsfolder }}/mco.yaml"
      when: not (use_operator | bool)

    # FIXME:
    # We apply the MCO template right away because that will cause the nodes to
    # reboot and only later we add the EBS shared volume
    # This is because GPFS refuses to read the /dev/disk/by-id/nvme-AMAZON..volid
    # symlink. This way we can just use /dev/nvme1n1 across the nodes
    - name: Apply MCO template
      tags:
        - 1_ocp_install_mco
      ansible.builtin.shell: |
        set -ex
        export KUBECONFIG=./auth/kubeconfig
        {{ oc_bin }} apply -f {{ gpfsfolder }}/mco.yaml
      args:
        chdir: "{{ ocpfolder }}"
      when: not (use_operator | bool)

    # FIXME(bandini): Sleep here for a bit so we're sure it is started
    # later we should wait for the mcp's to go in updating status and then
    # wait for the finished condition
    - name: Wait a bit for MCP to start
      tags:
        - 1_ocp_install_mco
      ansible.builtin.pause:
        minutes: 1
      when: not (use_operator | bool)

    - name: Speed up MCO
      tags:
        - 1_ocp_install_mco
      ansible.builtin.shell: |
        set -ex
        export KUBECONFIG=./auth/kubeconfig
        oc patch mcp worker --type merge --patch '{"spec": {"maxUnavailable": 2}}'
      args:
        chdir: "{{ ocpfolder }}"
      when: not (use_operator | bool)

    - name: Wait for MCP to settle
      tags:
        - 1_ocp_install_mco
      ansible.builtin.shell: |
        set -ex
        export KUBECONFIG=./auth/kubeconfig
        if [ $({{ oc_bin }} get mcp/master -o jsonpath='{.status.readyMachineCount}') != $({{ oc_bin }} get mcp/master -o jsonpath='{.status.machineCount}') ]; then
          exit 1
        fi
        if [ $({{ oc_bin }} get mcp/worker -o jsonpath='{.status.readyMachineCount}') != $({{ oc_bin }} get mcp/worker -o jsonpath='{.status.machineCount}') ]; then
          exit 1
        fi
      args:
        chdir: "{{ ocpfolder }}"
      retries: 30
      delay: 90
      register: mcp_ready
      until: mcp_ready is not failed
      when: not (use_operator | bool)

    # See https://www.ibm.com/docs/en/scalecontainernative/5.2.2?topic=aws-red-hat-openshift-configuration#authorize-rosa-worker-security-group-to-allow-ibm-storage-scale-container-native-ports
    - name: Gather security group info for workers
      tags:
        - 2_aws
      amazon.aws.ec2_security_group_info:
        region: "{{ ocp_region }}"
        filters:
          "tag:sigs.k8s.io/cluster-api-provider-aws/role": "node"
      register: sg_info

    - name: Debug sg_info
      tags:
        - 2_aws
      ansible.builtin.debug:
        msg: "{{ sg_info }}"

    - name: Set sg id
      tags:
        - 2_aws
      ansible.builtin.set_fact:
        sg_worker_id: "{{ sg_info.security_groups[0].group_id }}"

    # FIXME(bandini): use aws module here
    - name: Open up default security groups so gpfs can work in AWS
      tags:
        - 2_aws
      ansible.builtin.shell: |
        aws ec2 --region {{ ocp_region }} authorize-security-group-ingress --group-id {{ sg_worker_id }} --protocol tcp --port 12345 --source-group {{ sg_worker_id }}
        aws ec2 --region {{ ocp_region }} authorize-security-group-ingress --group-id {{ sg_worker_id }} --protocol tcp --port 1191 --source-group {{ sg_worker_id }}
        aws ec2 --region {{ ocp_region }} authorize-security-group-ingress --group-id {{ sg_worker_id }} --protocol tcp --port 60000-61000 --source-group {{ sg_worker_id }}

    # FIXME(bandini): this will need to be more robust
    - name: Find OpenShift EC2 Instances
      tags:
        - 3_ebs
      amazon.aws.ec2_instance_info:
        region: "{{ ocp_region }}"
        filters:
          "tag:Name": "{{ ocp_cluster_name }}*worker*"
          "instance-state-name": "running"
      register: ec2_workers

    - name: Set EC2 workers instance IDs
      tags:
        - 3_ebs
      ansible.builtin.set_fact:
        worker_ec2_ids: "{{ ec2_workers.instances | map(attribute='instance_id') | list }}"

    - name: Create EBS io2 volume
      tags:
        - 3_ebs
      amazon.aws.ec2_vol:
        region: "{{ ocp_region }}"
        availability_zone: "{{ ocp_az }}"
        volume_size: "{{ ebs_volume_size }}"
        volume_type: "{{ ebs_volume_type }}"
        multi_attach: yes
        iops: "{{ ebs_iops }}"
        tags:
          Name: "{{ gpfs_volume_name }}"
      register: ebs_volume

    - name: Attach EBS volume to workers
      tags:
        - 3_ebs
      amazon.aws.ec2_vol:
        region: "{{ ocp_region }}"
        instance: "{{ item }}"
        id: "{{ ebs_volume.volume_id }}"
        device_name: "{{ ebs_device_name }}"
      loop: "{{ worker_ec2_ids }}"
      when: ebs_volume.volume_id is defined

    # Installs GPFS without the purple operator
    - ansible.builtin.import_tasks: gpfs.yml
      when: not (use_operator | bool)

    # Installs GPFS with the purple operator
    - ansible.builtin.import_tasks: operator-install.yml
      when: (use_operator | bool)

    # Creates localdisk + filesystem + test environment
    - ansible.builtin.import_tasks: gpfs-day2.yml
