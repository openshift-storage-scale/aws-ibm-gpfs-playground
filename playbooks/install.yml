---
- name: Playbook to set up the Openshift Cluster
  hosts: localhost
  gather_facts: false
  become: false
  vars:
    use_operator: true
  vars_files:
    # Use this to override stuff that won't be committed to git
    - ../overrides.yml
  tasks:
    - name: Create working folder
      tags:
        - 1_ocp_install
      ansible.builtin.file:
        path: "{{ ocpfolder }}"
        state: directory
        recurse: true

    - name: Does cluster metadata.json exist
      tags:
        - 1_ocp_install
      ansible.builtin.stat:
        path: "{{ ocpfolder }}/metadata.json"
      register: metadata_json_file

    - name: Template OCP install file
      tags:
        - 1_ocp_install
      ansible.builtin.template:
        src: ../templates/full-cluster-install-config.j2.yaml
        dest: "{{ ocpfolder }}/install-config.yaml"
      when: not metadata_json_file.stat.exists

    - name: Install ocp cluster
      tags:
        - 1_ocp_install
      ansible.builtin.shell: |
        set -ex
        id
        {{ basefolder }}/{{ ocp_version }}/openshift-install create cluster --dir=. &> /tmp/oc-{{ ocp_version }}-{{ ocp_cluster_name }}.log
      args:
        chdir: "{{ ocpfolder }}"
      when: not metadata_json_file.stat.exists

    - name: Does cluster kubeconfig exist
      tags:
        - 1_ocp_install
      ansible.builtin.stat:
        path: "{{ ocpfolder }}/auth/kubeconfig"
      register: kubeconfig_file

    - name: Fail here there is no kubeconfig file
      tags:
        - 1_ocp_install
      fail: msg="The openshift cluster kubeconfig file is missing, please check {{ ocpfolder }}.openshift_install.log"
      when: not kubeconfig_file.stat.exists

    - name: Set kubeadmin password fact
      tags:
        - 1_ocp_install
      when: "{{ kubeadmin_pass | length > 0 }}"
      block:
        - name: Encrypt kubeadmin password
          ansible.builtin.shell: |
            set -o pipefail
            echo -n "{{ kubeadmin_pass }}" | htpasswd -i -B -n admin | awk -F ":" '{ print $2}' | base64 -w0
          register:
            hashed_password_out
        - name: Set hashed password fact
          tags:
            - 1_ocp_install
          ansible.builtin.set_fact:
            hashed_password: "{{ hashed_password_out.stdout }}"

        - name: Set kubeadmin password
          tags:
            - 1_ocp_install
          ansible.builtin.shell: |
            set -e
            chmod 0600 ./auth/kubeconfig
            export KUBECONFIG=./auth/kubeconfig
            {{ oc_bin }} patch secret -n kube-system kubeadmin --type json -p '[{"op": "replace", "path": "/data/kubeadmin", "value": "'{{ hashed_password }}'"}]'
          args:
            chdir: "{{ ocpfolder }}"

    # See https://www.ibm.com/docs/en/scalecontainernative/5.2.2?topic=aws-red-hat-openshift-configuration#authorize-rosa-worker-security-group-to-allow-ibm-storage-scale-container-native-ports
    - name: Gather security group info for workers
      tags:
        - 2_aws
      amazon.aws.ec2_security_group_info:
        region: "{{ ocp_region }}"
        filters:
          "tag:sigs.k8s.io/cluster-api-provider-aws/role": "node"
      register: sg_info

    - name: Debug sg_info
      tags:
        - 2_aws
      ansible.builtin.debug:
        msg: "{{ sg_info }}"

    - name: Set sg id
      tags:
        - 2_aws
      ansible.builtin.set_fact:
        sg_worker_id: "{{ sg_info.security_groups[0].group_id }}"

    # FIXME(bandini): use aws module here
    - name: Open up default security groups so gpfs can work in AWS
      tags:
        - 2_aws
      ansible.builtin.shell: |
        aws ec2 --region {{ ocp_region }} authorize-security-group-ingress --group-id {{ sg_worker_id }} --protocol tcp --port 12345 --source-group {{ sg_worker_id }}
        aws ec2 --region {{ ocp_region }} authorize-security-group-ingress --group-id {{ sg_worker_id }} --protocol tcp --port 1191 --source-group {{ sg_worker_id }}
        aws ec2 --region {{ ocp_region }} authorize-security-group-ingress --group-id {{ sg_worker_id }} --protocol tcp --port 60000-61000 --source-group {{ sg_worker_id }}

    # FIXME(bandini): this will need to be more robust
    - name: Find OpenShift EC2 Instances
      tags:
        - 3_ebs
      amazon.aws.ec2_instance_info:
        region: "{{ ocp_region }}"
        filters:
          "tag:Name": "{{ ocp_cluster_name }}*worker*"
          "instance-state-name": "running"
      register: ec2_workers

    - name: Set EC2 workers instance IDs
      tags:
        - 3_ebs
      ansible.builtin.set_fact:
        worker_ec2_ids: "{{ ec2_workers.instances | map(attribute='instance_id') | list }}"

    - name: Create EBS io2 volume
      tags:
        - 3_ebs
      amazon.aws.ec2_vol:
        region: "{{ ocp_region }}"
        availability_zone: "{{ ocp_az }}"
        volume_size: "{{ ebs_volume_size }}"
        volume_type: "{{ ebs_volume_type }}"
        multi_attach: yes
        iops: "{{ ebs_iops }}"
        tags:
          Name: "{{ gpfs_volume_name }}"
      register: ebs_volume

    - name: Attach EBS volume to workers
      tags:
        - 3_ebs
      amazon.aws.ec2_vol:
        region: "{{ ocp_region }}"
        instance: "{{ item }}"
        id: "{{ ebs_volume.volume_id }}"
        device_name: "{{ ebs_device_name }}"
      loop: "{{ worker_ec2_ids }}"
      when: ebs_volume.volume_id is defined

    # Installs GPFS without the openshift-fusion-access operator
    - ansible.builtin.import_tasks: gpfs.yml
      when: not (use_operator | bool)

    # Installs GPFS with the openshift-storage-scale operator
    - ansible.builtin.import_tasks: operator-install.yml
      when: (use_operator | bool)

    # Creates localdisk + filesystem + test environment
    - ansible.builtin.import_tasks: gpfs-day2.yml
