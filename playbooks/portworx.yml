---
- name: Playbook to set up the Portworx
  hosts: localhost
  gather_facts: false
  become: false
  vars:
    use_operator: true
  vars_files:
    # Use this to override stuff that won't be committed to git
    - ../overrides.yml
  tasks:
  - name: Get the Volume ID by Tag Name again
    tags:
      - 9_portworx
    amazon.aws.ec2_vol_info:
      region: "{{ ocp_region }}"
      filters:
        "tag:Name": "{{ gpfs_volume_name }}"
    register: volume_info
  
  - name: Fail if there is not exactly one ebs volume
    tags:
      - 9_portworx
    ansible.builtin.fail:
      msg: "There must be only one ebs volumes called {{ gpfs_volume_name }}: {{ volume_info }}"
    when: volume_info.volumes | length != 1
  
  - name: Set volumeid fact
    tags:
      - 9_portworx
    ansible.builtin.set_fact:
      ebs_volid: "{{ volume_info.volumes[0].id | replace('-', '') }}"
  
  - name: Debug volumeid fact
    tags:
      - 9_portworx
    ansible.builtin.debug:
      msg: "{{ ebs_volid }}"
  
  - name: Get worker nodes names
    tags:
      - 9_portworx
    ansible.builtin.shell: |
      export KUBECONFIG=./auth/kubeconfig
      {{ oc_bin }} get nodes -l node-role.kubernetes.io/worker -o name | cut -f2 -d/
    args:
      chdir: "{{ ocpfolder }}"
    register: worker_nodes_output
  
  - name: Set worker nodes names fact
    tags:
      - 9_portworx
    ansible.builtin.set_fact:
      worker_nodes: "{{ worker_nodes_output.stdout_lines }}"
  
  # This actually works for any worker when using the symlink
  - name: Set device name for worker_0
    tags:
      - 9_portworx
    ansible.builtin.set_fact:
      realdevice: "/dev/disk/by-id/nvme-Amazon_Elastic_Block_Store_{{ ebs_volid }}"

  - name: Template portworx stuff
    tags:
      9_portworx
    ansible.builtin.template:
      src: ../templates/{{ item }}
      dest: "{{ gpfsfolder }}/{{ item }}"
    loop:
      - portworx-subscription.yaml
      - portworx-storagecluster.yaml

    # See https://www.ibm.com/docs/en/scalecontainernative/5.2.2?topic=aws-red-hat-openshift-configuration#authorize-rosa-worker-security-group-to-allow-ibm-storage-scale-container-native-ports
  - name: Gather security group info for workers
    tags:
      - 9_portworx
    amazon.aws.ec2_security_group_info:
      profile: "{{ aws_profile }}"
      region: "{{ ocp_region }}"
      filters:
        "tag:sigs.k8s.io/cluster-api-provider-aws/role": "node"
    register: sg_info

  - name: Debug sg_info
    tags:
      - 9_portworx
    ansible.builtin.debug:
      msg: "{{ sg_info }}"

  - name: Set sg id
    tags:
      - 9_portworx
    ansible.builtin.set_fact:
      sg_worker_id: "{{ sg_info.security_groups[0].group_id }}"

  # FIXME(bandini): use aws module here
  # When using baremetal these fail so let's not exit here in any case
  - name: Open up default security groups so gpfs can work in AWS
    tags:
      - 9_portworx
    ansible.builtin.shell: |
      aws --profile {{ aws_profile }} ec2 --region {{ ocp_region }} authorize-security-group-ingress --group-id {{ sg_worker_id }} --protocol tcp --port 17001-17020 --source-group {{ sg_worker_id }}
      aws --profile {{ aws_profile }} ec2 --region {{ ocp_region }} authorize-security-group-ingress --group-id {{ sg_worker_id }} --protocol udp --port 17002 --source-group {{ sg_worker_id }}
    failed_when: false
          
  - name: Apply portworx subscription
    ansible.builtin.shell: |
      set -e
      export KUBECONFIG=./auth/kubeconfig
      {{ oc_bin }} apply -f "{{ gpfsfolder }}/virt-support.yaml"
    args:
      chdir: "{{ ocpfolder }}"
