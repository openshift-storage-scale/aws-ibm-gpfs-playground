# we need to label the workers so the localdisk
- name: Label the workers
  tags:
    - 6_gpfs
  ansible.builtin.shell: |
    set -ex
    export KUBECONFIG={{ kubeconfig }}
    for node in $({{ oc_bin }} get nodes -l node-role.kubernetes.io/worker -o name)
    do
      {{ oc_bin }} label ${node} scale.spectrum.ibm.com/role=storage
      {{ oc_bin }} label ${node} scale.spectrum.ibm.com/daemon-selector=""
    done

- name: Template the Fusion Cluster object
  ansible.builtin.template:
    src: ../templates/cluster.yaml
    dest: "{{ gpfsfolder }}/cluster.yaml"
  tags:
    - 6_gpfs

- name: Apply the Fusion cluster object
  tags:
    - 6_gpfs
  ansible.builtin.shell: |
    set -ex
    export KUBECONFIG={{ kubeconfig }}
    {{ oc_bin }} apply -f "{{ gpfsfolder }}/cluster.yaml"
  register: fusion_apply
  until: fusion_apply is not failed
  retries: 20
  delay: 20

- name: Wait for fusion pods
  tags:
    - 6_gpfs
  ansible.builtin.shell: |
    set -ex
    export KUBECONFIG={{ kubeconfig }}
    COUNT=$({{ oc_bin }} get pods -n ibm-spectrum-scale -l app.kubernetes.io/name=core --no-headers | grep Running | wc -l)
    NODES=$(oc get nodes -l "node-role.kubernetes.io/worker" --no-headers | wc -l)
    if [ ${COUNT} != ${NODES} ]; then
      exit 1
    fi
  register: fusion_apply
  until: fusion_apply is not failed
  retries: 30
  delay: 30

- name: Get the Volume ID by Tag Name again
  tags:
    - 6_gpfs
  amazon.aws.ec2_vol_info:
    profile: "{{ aws_profile }}"
    region: "{{ ocp_region }}"
    filters:
      "tag:Name": "{{ gpfs_volume_name }}"
  register: volume_info

- name: Fail if there are no ebs volumes
  tags:
    - 6_gpfs
  ansible.builtin.fail:
    msg: "There must be at least one ebs volume called {{ gpfs_volume_name }}: {{ volume_info }}"
  when: volume_info.volumes | length == 0

- name: Build list of in-use volumes
  tags:
    - 6_gpfs
  ansible.builtin.set_fact:
    in_use_volumes: "{{ volume_info.volumes | selectattr('status', 'equalto', 'in-use') | list }}"

- name: Set volumeid fact (use in-use volume if multiple exist)
  tags:
    - 6_gpfs
  ansible.builtin.set_fact:
    ebs_volid: "{{ (in_use_volumes | first | default(volume_info.volumes[0])).id | replace('-', '') }}"

- name: Debug volumeid fact
  tags:
    - 6_gpfs
  ansible.builtin.debug:
    msg: "{{ ebs_volid }}"

- name: Get worker nodes names
  tags:
    - 6_gpfs
  ansible.builtin.shell: |
    export KUBECONFIG={{ kubeconfig }}
    {{ oc_bin }} get nodes -l node-role.kubernetes.io/worker -o name | cut -f2 -d/
  register: worker_nodes_output

- name: Set worker nodes names fact
  tags:
    - 6_gpfs
  ansible.builtin.set_fact:
    worker_nodes: "{{ worker_nodes_output.stdout_lines }}"

- name: Check if FileSystemClaim already exists
  tags:
    - 6_gpfs
  ansible.builtin.shell: |
    set -e
    export KUBECONFIG={{ kubeconfig }}
    {{ oc_bin }} get filesystemclaim -n ibm-spectrum-scale filesystemclaim-sample 2>/dev/null
  register: fsc_check
  failed_when: false
  changed_when: false

- name: Debug FSC check result
  tags:
    - 6_gpfs
  ansible.builtin.debug:
    msg: "FileSystemClaim exists: {{ fsc_check.rc == 0 }} (rc={{ fsc_check.rc }})"

- name: Query LocalVolumeDiscoveryResult to get device path
  tags:
    - 6_gpfs
  ansible.builtin.shell: |
    set -ex
    export KUBECONFIG={{ kubeconfig }}
    {{ oc_bin }} get localvolumediscoveryresult -n {{ operator_namespace }} -o json
  register: lvdr_output
  retries: 20
  delay: 30
  until: 
    - lvdr_output is not failed
    - (lvdr_output.stdout | from_json)['items'] | length > 0
    - (lvdr_output.stdout | from_json)['items'][0].status.discoveredDevices is defined
    - (lvdr_output.stdout | from_json)['items'][0].status.discoveredDevices | length > 0
  when: fsc_check.rc != 0

- name: Parse LVDR and extract device path
  tags:
    - 6_gpfs
  ansible.builtin.set_fact:
    lvdr_json: "{{ lvdr_output.stdout | from_json }}"
  when: fsc_check.rc != 0

- name: Fail if no devices discovered
  tags:
    - 6_gpfs
  ansible.builtin.fail:
    msg: "No devices discovered in LocalVolumeDiscoveryResult for volume {{ gpfs_volume_name }}"
  when:
    - fsc_check.rc != 0
    - lvdr_json['items'][0].status.discoveredDevices | length == 0

- name: Debug LVDR structure
  tags:
    - 6_gpfs
  ansible.builtin.debug:
    msg: 
      - "LVDR items count: {{ lvdr_json['items'] | length }}"
      - "First LVDR node: {{ lvdr_json['items'][0].spec.nodeName }}"
      - "Discovered devices count: {{ lvdr_json['items'][0].status.discoveredDevices | length }}"
      - "First discovered device details: {{ lvdr_json['items'][0].status.discoveredDevices[0] }}"
      - "WWN value: {{ lvdr_json['items'][0].status.discoveredDevices[0].get('WWN', 'NOT FOUND') }}"
  when: fsc_check.rc != 0

- name: Extract first discovered device
  tags:
    - 6_gpfs
  ansible.builtin.set_fact:
    discovered_device: "{{ lvdr_json['items'][0].status.discoveredDevices[0] }}"
  when: fsc_check.rc != 0

- name: Set device WWN from first discovered device
  tags:
    - 6_gpfs
  ansible.builtin.set_fact:
    ebs_device_path: "{{ discovered_device.get('WWN', discovered_device.path) }}"
  when: fsc_check.rc != 0

- name: Debug device path
  tags:
    - 6_gpfs
  ansible.builtin.debug:
    msg: "Using device identifier: {{ ebs_device_path }}"
  when: fsc_check.rc != 0

- name: Template the FileSystemClaim
  tags:
    - 6_gpfs
  ansible.builtin.template:
    src: ../templates/filesystemclaim.yaml
    dest: "{{ gpfsfolder }}/filesystemclaim.yaml"
  when: fsc_check.rc != 0

- name: Apply the FileSystemClaim
  tags:
    - 6_gpfs
  ansible.builtin.shell: |
    set -ex
    export KUBECONFIG={{ kubeconfig }}
    {{ oc_bin }} apply -f "{{ gpfsfolder }}/filesystemclaim.yaml"
  retries: 10
  delay: 30
  register: filesystemclaim_ready
  until: filesystemclaim_ready is not failed
  when: fsc_check.rc != 0

- name: Wait for FileSystemClaim to be ready
  tags:
    - 7_gpfs
  ansible.builtin.shell: |
    set -ex
    export KUBECONFIG={{ kubeconfig }}
    {{ oc_bin }} get filesystemclaim -n ibm-spectrum-scale filesystemclaim-sample -o jsonpath='{.status.conditions[?(@.type=="Ready")].status}' | grep -q "True"
  retries: 30
  delay: 30
  register: filesystemclaim_ready
  until: filesystemclaim_ready is not failed
  when: fsc_check.rc != 0

- name: Template the snapshotclass
  tags:
    - 7_gpfs
  ansible.builtin.template:
    src: ../templates/snapshot.yaml
    dest: "{{ gpfsfolder }}/snapshot.yaml"

- name: Apply the snapshotclass
  tags:
    - 7_gpfs
  ansible.builtin.shell: |
    set -ex
    export KUBECONFIG={{ kubeconfig }}
    {{ oc_bin }} apply -f "{{ gpfsfolder }}/snapshot.yaml"

- name: Template the test deployment
  tags:
    - 8_gpfs
  ansible.builtin.template:
    src: ../templates/test_consume.yaml
    dest: "{{ gpfsfolder }}/test_consume.yaml"

- name: Apply the test deployment
  tags:
    - 8_gpfs
  ansible.builtin.shell: |
    set -ex
    export KUBECONFIG={{ kubeconfig }}
    {{ oc_bin }} apply -f "{{ gpfsfolder }}/test_consume.yaml"

- block:
  - name: Get the Volume ID by Tag Name again (2)
    tags:
      - 6_gpfs
    amazon.aws.ec2_vol_info:
      profile: "{{ aws_profile }}"
      region: "{{ ocp_region }}"
      filters:
        "tag:Name": "{{ gpfs_volume_name_two }}"
    register: volume_info_two

  - name: Fail if there are no ebs volumes (2)
    tags:
      - 6_gpfs
    ansible.builtin.fail:
      msg: "There must be at least one ebs volume called {{ gpfs_volume_name_two }}: {{ volume_info_two }}"
    when: volume_info_two.volumes | length == 0

  - name: Build list of in-use volumes (2)
    tags:
      - 6_gpfs
    ansible.builtin.set_fact:
      in_use_volumes_two: "{{ volume_info_two.volumes | selectattr('status', 'equalto', 'in-use') | list }}"

  - name: Set volumeid fact (2) (use in-use volume if multiple exist)
    tags:
      - 6_gpfs
    ansible.builtin.set_fact:
      ebs_volid_two: "{{ (in_use_volumes_two | first | default(volume_info_two.volumes[0])).id | replace('-', '') }}"

  - name: Debug volumeid fact
    tags:
      - 6_gpfs
    ansible.builtin.debug:
      msg: "{{ ebs_volid_two }}"

  # This actually works for any worker when using the symlink
  - name: Set device name for worker_0 (2)
    tags:
      - 6_gpfs
    ansible.builtin.set_fact:
      realdevice_two: "/dev/disk/by-id/nvme-Amazon_Elastic_Block_Store_{{ ebs_volid_two }}"

  - name: Template the localdisk (2)
    tags:
      - 6_gpfs
    ansible.builtin.template:
      src: ../templates/localdisk2.yaml
      dest: "{{ gpfsfolder }}/localdisk2.yaml"

  - name: Apply the localdisk (2)
    tags:
      - 6_gpfs
    ansible.builtin.shell: |
      set -ex
      export KUBECONFIG={{ kubeconfig }}
      {{ oc_bin }} apply -f "{{ gpfsfolder }}/localdisk2.yaml"
    retries: 10
    delay: 30
    register: localdisk_ready
    until: localdisk_ready is not failed

  - name: Template the filesystem
    tags:
      - 7_gpfs
    ansible.builtin.template:
      src: ../templates/filesystem2.yaml
      dest: "{{ gpfsfolder }}/filesystem2.yaml"

  - name: Apply the filesystem (2)
    tags:
      - 7_gpfs
    ansible.builtin.shell: |
      set -ex
      export KUBECONFIG={{ kubeconfig }}
      {{ oc_bin }} apply -f "{{ gpfsfolder }}/filesystem2.yaml"
    retries: 10
    delay: 30
    register: filesystem_ready
    until: filesystem_ready is not failed

  - name: Wait for the filesystem to be ready (2)
    tags:
      - 7_gpfs
    ansible.builtin.shell: |
      set -ex
      export KUBECONFIG={{ kubeconfig }}
      {{ oc_bin }} get filesystem -n ibm-spectrum-scale {{ gpfs_fs_name_two }} -o jsonpath='{.status.pools[0].totalDiskSize}' | grep "{{ ebs_volume_size_two }}"
    retries: 15
    delay: 30
    register: filesystem_ready
    until: filesystem_ready is not failed

  - name: Template the snapshotclass and storageclass
    tags:
      - 7_gpfs
    ansible.builtin.template:
      src: ../templates/{{ item }}
      dest: "{{ gpfsfolder }}/{{ item }}"
    loop:
      - storageclass2.yaml

  - name: Apply the snapshotclass and storageclass
    tags:
      - 7_gpfs
    ansible.builtin.shell: |
      set -ex
      export KUBECONFIG={{ kubeconfig }}
      {{ oc_bin }} apply -f "{{ gpfsfolder }}/{{ item }}"
    loop:
      - storageclass2.yaml
  when: baremetal_env | bool
