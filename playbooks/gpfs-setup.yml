# we need to label the workers so the localdisk
- name: Label the workers
  tags:
    - 6_gpfs
  ansible.builtin.shell: |
    set -ex
    export KUBECONFIG={{ kubeconfig }}
    for node in $({{ oc_bin }} get nodes -l node-role.kubernetes.io/worker -o name)
    do
      {{ oc_bin }} label ${node} scale.spectrum.ibm.com/role=storage
      {{ oc_bin }} label ${node} scale.spectrum.ibm.com/daemon-selector=""
    done

- name: Template the Fusion Cluster object
  ansible.builtin.template:
    src: ../templates/cluster.yaml
    dest: "{{ gpfsfolder }}/cluster.yaml"
  tags:
    - 6_gpfs

- name: Apply the Fusion cluster object
  tags:
    - 6_gpfs
  ansible.builtin.shell: |
    set -ex
    export KUBECONFIG={{ kubeconfig }}
    {{ oc_bin }} apply -f "{{ gpfsfolder }}/cluster.yaml"
  register: fusion_apply
  until: fusion_apply is not failed
  retries: 20
  delay: 20

- name: Wait for fusion pods
  tags:
    - 6_gpfs
  ansible.builtin.shell: |
    set -e
    export KUBECONFIG={{ kubeconfig }}
    COUNT=$({{ oc_bin }} get pods -n ibm-spectrum-scale -l app.kubernetes.io/name=core --no-headers | grep Running | wc -l)
    NODES=$(oc get nodes -l "node-role.kubernetes.io/worker" --no-headers | wc -l)
    if [ ${COUNT} != ${NODES} ]; then
      exit 1
    fi
  register: fusion_apply
  until: fusion_apply is not failed
  retries: 30
  delay: 30

- name: Get the Volume ID by Tag Name again
  tags:
    - 6_gpfs
  amazon.aws.ec2_vol_info:
    profile: "{{ aws_profile }}"
    region: "{{ ocp_region }}"
    filters:
      "tag:Name": "{{ gpfs_volume_name }}"
  register: volume_info

- name: Fail if there is not exactly one ebs volume
  tags:
    - 6_gpfs
  ansible.builtin.fail:
    msg: "There must be only one ebs volumes called {{ gpfs_volume_name }}: {{ volume_info }}"
  when: volume_info.volumes | length != 1

- name: Set volumeid fact
  tags:
    - 6_gpfs
  ansible.builtin.set_fact:
    ebs_volid: "{{ volume_info.volumes[0].id | replace('-', '') }}"

- name: Debug volumeid fact
  tags:
    - 6_gpfs
  ansible.builtin.debug:
    msg: "{{ ebs_volid }}"

- name: Get worker nodes names
  tags:
    - 6_gpfs
  ansible.builtin.shell: |
    export KUBECONFIG={{ kubeconfig }}
    {{ oc_bin }} get nodes -l node-role.kubernetes.io/worker -o name | cut -f2 -d/
  register: worker_nodes_output

- name: Set worker nodes names fact
  tags:
    - 6_gpfs
  ansible.builtin.set_fact:
    worker_nodes: "{{ worker_nodes_output.stdout_lines }}"

# This actually works for any worker when using the symlink
- name: Set device name for worker_0
  tags:
    - 6_gpfs
  ansible.builtin.set_fact:
    realdevice: "/dev/disk/by-id/nvme-Amazon_Elastic_Block_Store_{{ ebs_volid }}"

- name: Template the localdisk
  tags:
    - 6_gpfs
  ansible.builtin.template:
    src: ../templates/localdisk.yaml
    dest: "{{ gpfsfolder }}/localdisk.yaml"

- name: Apply the localdisk
  tags:
    - 6_gpfs
  ansible.builtin.shell: |
    set -ex
    export KUBECONFIG={{ kubeconfig }}
    {{ oc_bin }} apply -f "{{ gpfsfolder }}/localdisk.yaml"
  retries: 10
  delay: 30
  register: localdisk_ready
  until: localdisk_ready is not failed

- name: Template the filesystem
  tags:
    - 7_gpfs
  ansible.builtin.template:
    src: ../templates/filesystem.yaml
    dest: "{{ gpfsfolder }}/filesystem.yaml"

- name: Apply the filesystem
  tags:
    - 7_gpfs
  ansible.builtin.shell: |
    set -ex
    export KUBECONFIG={{ kubeconfig }}
    {{ oc_bin }} apply -f "{{ gpfsfolder }}/filesystem.yaml"
  retries: 10
  delay: 30
  register: filesystem_ready
  until: filesystem_ready is not failed

- name: Wait for the filesystem to be ready
  tags:
    - 7_gpfs
  ansible.builtin.shell: |
    set -ex
    export KUBECONFIG={{ kubeconfig }}
    {{ oc_bin }} get filesystem -n ibm-spectrum-scale {{ gpfs_fs_name }} -o jsonpath='{.status.pools[0].totalDiskSize}' | grep "{{ ebs_volume_size }}"
  retries: 15
  delay: 30
  register: filesystem_ready
  until: filesystem_ready is not failed

- name: Template the snapshotclass and storageclass
  tags:
    - 7_gpfs
  ansible.builtin.template:
    src: ../templates/{{ item }}
    dest: "{{ gpfsfolder }}/{{ item }}"
  loop:
    - snapshot.yaml
    - storageclass.yaml

- name: Apply the snapshotclass and storageclass
  tags:
    - 7_gpfs
  ansible.builtin.shell: |
    set -ex
    export KUBECONFIG={{ kubeconfig }}
    {{ oc_bin }} apply -f "{{ gpfsfolder }}/{{ item }}"
  loop:
    - snapshot.yaml
    - storageclass.yaml

- name: Template the test deployment
  tags:
    - 8_gpfs
  ansible.builtin.template:
    src: ../templates/test_consume.yaml
    dest: "{{ gpfsfolder }}/test_consume.yaml"

- name: Apply the test deployment
  tags:
    - 8_gpfs
  ansible.builtin.shell: |
    set -ex
    export KUBECONFIG={{ kubeconfig }}
    {{ oc_bin }} apply -f "{{ gpfsfolder }}/test_consume.yaml"

- block:
  - name: Get the Volume ID by Tag Name again (2)
    tags:
      - 6_gpfs
    amazon.aws.ec2_vol_info:
      profile: "{{ aws_profile }}"
      region: "{{ ocp_region }}"
      filters:
        "tag:Name": "{{ gpfs_volume_name_two }}"
    register: volume_info_two

  - name: Fail if there is not exactly one ebs volume (2)
    tags:
      - 6_gpfs
    ansible.builtin.fail:
      msg: "There must be only one ebs volumes called {{ gpfs_volume_name }}: {{ volume_info }}"
    when: volume_info_two.volumes | length != 1

  - name: Set volumeid fact
    tags:
      - 6_gpfs
    ansible.builtin.set_fact:
      ebs_volid_two: "{{ volume_info_two.volumes[0].id | replace('-', '') }}"

  - name: Debug volumeid fact
    tags:
      - 6_gpfs
    ansible.builtin.debug:
      msg: "{{ ebs_volid_two }}"

  # This actually works for any worker when using the symlink
  - name: Set device name for worker_0 (2)
    tags:
      - 6_gpfs
    ansible.builtin.set_fact:
      realdevice_two: "/dev/disk/by-id/nvme-Amazon_Elastic_Block_Store_{{ ebs_volid_two }}"

  - name: Template the localdisk (2)
    tags:
      - 6_gpfs
    ansible.builtin.template:
      src: ../templates/localdisk2.yaml
      dest: "{{ gpfsfolder }}/localdisk2.yaml"

  - name: Apply the localdisk (2)
    tags:
      - 6_gpfs
    ansible.builtin.shell: |
      set -ex
      export KUBECONFIG={{ kubeconfig }}
      {{ oc_bin }} apply -f "{{ gpfsfolder }}/localdisk2.yaml"
    retries: 10
    delay: 30
    register: localdisk_ready
    until: localdisk_ready is not failed

  - name: Template the filesystem
    tags:
      - 7_gpfs
    ansible.builtin.template:
      src: ../templates/filesystem2.yaml
      dest: "{{ gpfsfolder }}/filesystem2.yaml"

  - name: Apply the filesystem (2)
    tags:
      - 7_gpfs
    ansible.builtin.shell: |
      set -ex
      export KUBECONFIG={{ kubeconfig }}
      {{ oc_bin }} apply -f "{{ gpfsfolder }}/filesystem2.yaml"
    retries: 10
    delay: 30
    register: filesystem_ready
    until: filesystem_ready is not failed

  - name: Wait for the filesystem to be ready (2)
    tags:
      - 7_gpfs
    ansible.builtin.shell: |
      set -ex
      export KUBECONFIG={{ kubeconfig }}
      {{ oc_bin }} get filesystem -n ibm-spectrum-scale {{ gpfs_fs_name_two }} -o jsonpath='{.status.pools[0].totalDiskSize}' | grep "{{ ebs_volume_size_two }}"
    retries: 15
    delay: 30
    register: filesystem_ready
    until: filesystem_ready is not failed

  - name: Template the snapshotclass and storageclass
    tags:
      - 7_gpfs
    ansible.builtin.template:
      src: ../templates/{{ item }}
      dest: "{{ gpfsfolder }}/{{ item }}"
    loop:
      - storageclass2.yaml

  - name: Apply the snapshotclass and storageclass
    tags:
      - 7_gpfs
    ansible.builtin.shell: |
      set -ex
      export KUBECONFIG={{ kubeconfig }}
      {{ oc_bin }} apply -f "{{ gpfsfolder }}/{{ item }}"
    loop:
      - storageclass2.yaml
  when: virt_enabled | bool
