# we need to label the workers so the localdisk
- name: Label the workers
  tags:
    - 6_gpfs
  ansible.builtin.shell: |
    set -ex
    export KUBECONFIG=./auth/kubeconfig
    for node in $({{ oc_bin }} get nodes -l node-role.kubernetes.io/worker -o name)
    do
      {{ oc_bin }} label ${node} scale.spectrum.ibm.com/role=storage
      {{ oc_bin }} label ${node} scale.spectrum.ibm.com/daemon-selector=""
    done
  args:
    chdir: "{{ ocpfolder }}"

- name: Get the Volume ID by Tag Name again
  tags:
    - 6_gpfs
  amazon.aws.ec2_vol_info:
    region: "{{ ocp_region }}"
    filters:
      "tag:Name": "{{ gpfs_volume_name }}"
  register: volume_info

- name: Fail if there is not exactly one ebs volume
  tags:
    - 6_gpfs
  ansible.builtin.fail:
    msg: "There must be only one ebs volumes called {{ gpfs_volume_name }}: {{ volume_info }}"
  when: volume_info.volumes | length != 1

- name: Set volumeid fact
  tags:
    - 6_gpfs
  ansible.builtin.set_fact:
    ebs_volid: "{{ volume_info.volumes[0].id | replace('-', '') }}"

- name: Debug volumeid fact
  tags:
    - 6_gpfs
  ansible.builtin.debug:
    msg: "{{ ebs_volid }}"

- name: Get worker nodes names
  tags:
    - 6_gpfs
  ansible.builtin.shell: |
    export KUBECONFIG=./auth/kubeconfig
    {{ oc_bin }} get nodes -l node-role.kubernetes.io/worker -o name | cut -f2 -d/
  args:
    chdir: "{{ ocpfolder }}"
  register: worker_nodes_output

- name: Set worker nodes names fact
  tags:
    - 6_gpfs
  ansible.builtin.set_fact:
    worker_nodes: "{{ worker_nodes_output.stdout_lines }}"

# This actually works for any worker when using the symlink
- name: Set device name for worker_0
  tags:
    - 6_gpfs
  ansible.builtin.set_fact:
    realdevice: "/dev/disk/by-id/nvme-Amazon_Elastic_Block_Store_{{ ebs_volid }}"

- name: Template the localdisk
  tags:
    - 6_gpfs
  ansible.builtin.template:
    src: ../templates/localdisk.yaml
    dest: "{{ gpfsfolder }}/localdisk.yaml"

- name: Apply the localdisk
  tags:
    - 6_gpfs
  ansible.builtin.shell: |
    set -ex
    export KUBECONFIG=./auth/kubeconfig
    {{ oc_bin }} apply -f "{{ gpfsfolder }}/localdisk.yaml"
  args:
    chdir: "{{ ocpfolder }}"
  retries: 10
  delay: 30
  register: localdisk_ready
  until: localdisk_ready is not failed

- name: Template the filesystem
  tags:
    - 7_gpfs
  ansible.builtin.template:
    src: ../templates/filesystem.yaml
    dest: "{{ gpfsfolder }}/filesystem.yaml"

- name: Apply the filesystem
  tags:
    - 7_gpfs
  ansible.builtin.shell: |
    set -ex
    export KUBECONFIG=./auth/kubeconfig
    {{ oc_bin }} apply -f "{{ gpfsfolder }}/filesystem.yaml"
  args:
    chdir: "{{ ocpfolder }}"
  retries: 10
  delay: 30
  register: filesystem_ready
  until: filesystem_ready is not failed

- name: Template our own storageclass
  tags:
    - 7_gpfs
  ansible.builtin.template:
    src: ../templates/storageclass.yaml
    dest: "{{ gpfsfolder }}/storageclass.yaml"

- name: Apply the storageclass
  tags:
    - 7_gpfs
  ansible.builtin.shell: |
    set -ex
    export KUBECONFIG=./auth/kubeconfig
    {{ oc_bin }} apply -f "{{ gpfsfolder }}/storageclass.yaml"
  args:
    chdir: "{{ ocpfolder }}"
  retries: 10
  delay: 30
  register: storageclass_ready
  until: storageclass_ready is not failed

- name: Template the snapshotclass
  tags:
    - 7_gpfs
  ansible.builtin.template:
    src: ../templates/snapshot.yaml
    dest: "{{ gpfsfolder }}/snapshot.yaml"

- name: Apply the snapshotclass
  tags:
    - 7_gpfs
  ansible.builtin.shell: |
    set -ex
    export KUBECONFIG=./auth/kubeconfig
    {{ oc_bin }} apply -f "{{ gpfsfolder }}/snapshot.yaml"
  args:
    chdir: "{{ ocpfolder }}"

- name: Template the test deployment
  tags:
    - 8_gpfs
  ansible.builtin.template:
    src: ../templates/test_consume.yaml
    dest: "{{ gpfsfolder }}/test_consume.yaml"

- name: Apply the test deployment
  tags:
    - 8_gpfs
  ansible.builtin.shell: |
    set -ex
    export KUBECONFIG=./auth/kubeconfig
    {{ oc_bin }} apply -f "{{ gpfsfolder }}/test_consume.yaml"
  args:
    chdir: "{{ ocpfolder }}"

